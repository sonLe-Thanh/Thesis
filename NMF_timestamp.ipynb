{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "03d8a7ed-ac54-4cee-8d44-1821055de73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d132cfd4-0f23-4a8c-bc9a-cc86944b1914",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa7ac9db-8532-4a9b-9154-d996ee2b4204",
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNTRY = [\"China\", \"France\", \"Germany\", \"Japan\", \"Korea\", \"Vietnam\"]\n",
    "TIMESTAMP = [\"pre2018\", \"2018\", \"2019\", \"2020\", \"2021\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0add8a65-bec5-4cfc-9eca-045d5d1ba708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(country=\"China\", timestamp=\"2021\", test_size=0.2, min_topic=40, max_topic=100):\n",
    "    # read data\n",
    "    data_file = \"Data/Timestamp/\" + country + \"_\" + timestamp + \".txt\"\n",
    "    with open(data_file) as file:\n",
    "        lines = file.readlines()\n",
    "    print(\"Read data from path: \" + data_file)\n",
    "    \n",
    "    # tokenize\n",
    "    corpus = [list(gensim.utils.tokenize(line, deacc = True)) for line in lines]\n",
    "    \n",
    "    # get corpus\n",
    "    corpus = pd.DataFrame({'reviews': corpus})\n",
    "    docs = corpus['reviews']\n",
    "    \n",
    "    # # train test split\n",
    "    # train, test = train_test_split(corpus, test_size=test_size)\n",
    "    # docs, docs_train, docs_test = corpus['reviews'], train['reviews'], test['reviews']\n",
    "    \n",
    "    # vocabulary / dictionary\n",
    "    dictionary = gensim.corpora.Dictionary(docs)\n",
    "    \n",
    "    # BOW\n",
    "    bow = [dictionary.doc2bow(doc) for doc in docs]\n",
    "    # bow_train = [dictionary.doc2bow(doc) for doc in docs_train]\n",
    "    # bow_test = [dictionary.doc2bow(doc) for doc in docs_test]\n",
    "    \n",
    "    # tfidf\n",
    "    tfidf = gensim.models.TfidfModel(bow)\n",
    "    # tfidf_train = gensim.models.TfidfModel(bow_train)\n",
    "    # tfidf_test = gensim.models.TfidfModel(bow_test)\n",
    "\n",
    "    tfidf = tfidf[bow]\n",
    "    # tfidf_train = tfidf_train[bow_train]\n",
    "    # tfidf_test = tfidf_test[bow_test]\n",
    "    \n",
    "    # set up file for saving data\n",
    "    directory = \"Data/NMF/Result_timestamp/\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    file_name_umass = directory + country + \"_\" + timestamp + \"_result_umass.csv\"\n",
    "    # file_name_perplexity = directory + country + \"_\" + timestamp + \"_result_perplexity.csv\"\n",
    "    # file_name_perplexity_train = \"Data/Result_timestamp/\" + country + \"_\" + timestamp + \"_\" + str(no_iterations) +\"_result_perplexity_train.csv\"\n",
    "    # file_name_perplexity_test = \"Data/Result_timestamp/\" + country + \"_\" + timestamp + \"_\" + str(no_iterations) +\"_result_perplexity_test.csv\"\n",
    "\n",
    "    file_write_umass = open(file_name_umass, \"a\")\n",
    "    # file_write_perplexity = open(file_name_perplexity, \"a\")\n",
    "    # file_write_perplexity_train = open(file_name_perplexity_train, \"a\")\n",
    "    # file_write_perplexity_test = open(file_name_perplexity_test, \"a\")\n",
    "\n",
    "    for no_topics in range(min_topic,max_topic + 1):\n",
    "\n",
    "        start = time.time()\n",
    "        nmf_model_tfidf = gensim.models.nmf.Nmf(tfidf, \n",
    "                                                 num_topics=no_topics, \n",
    "                                                 id2word = dictionary, \n",
    "                                                 passes = 2)\n",
    "        end = time.time()\n",
    "        \n",
    "        # save umass result\n",
    "        u_mass = CoherenceModel(model=nmf_model_tfidf, corpus=bow, dictionary=dictionary ,coherence='u_mass', topn=20)\n",
    "        u_mass_res = u_mass.get_coherence() \n",
    "\n",
    "        # save perplexity result\n",
    "        # perplexity = nmf_model_tfidf.log_perplexity(tfidf, len(docs))\n",
    "        # perplexity_train = lda_model_tfidf.log_perplexity(tfidf_train, len(docs_train))\n",
    "        # perplexity_test = lda_model_tfidf.log_perplexity(tfidf_test, len(docs_test))\n",
    "        \n",
    "        # store data to file\n",
    "        file_write_umass.write(str(no_topics) + \",\" + str(u_mass_res) +\",\"+str(end-start)+\"\\n\")\n",
    "        # file_write_perplexity.write(str(no_topics) + \",\" + str(perplexity) +\",\"+str(end - start)+\"\\n\")\n",
    "        # file_write_perplexity_train.write(str(no_topics) + \",\" + str(perplexity_train) +\",\"+str(end - start)+\"\\n\")\n",
    "        # file_write_perplexity_test.write(str(no_topics) + \",\" + str(perplexity_test) +\",\"+str(end - start)+\"\\n\")\n",
    "\n",
    "    file_write_umass.close()\n",
    "    # file_write_perplexity.close()\n",
    "    # file_write_perplexity_train.close()\n",
    "    # file_write_perplexity_test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4790295c-9687-4285-8fc2-ceb1ce11eb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read data from path: Data/Timestamp/China_pre2018.txt\n",
      "Read data from path: Data/Timestamp/China_2018.txt\n",
      "Read data from path: Data/Timestamp/China_2019.txt\n",
      "Read data from path: Data/Timestamp/China_2020.txt\n",
      "Read data from path: Data/Timestamp/China_2021.txt\n",
      "Read data from path: Data/Timestamp/France_pre2018.txt\n",
      "Read data from path: Data/Timestamp/France_2018.txt\n",
      "Read data from path: Data/Timestamp/France_2019.txt\n",
      "Read data from path: Data/Timestamp/France_2020.txt\n",
      "Read data from path: Data/Timestamp/France_2021.txt\n",
      "Read data from path: Data/Timestamp/Germany_pre2018.txt\n",
      "Read data from path: Data/Timestamp/Germany_2018.txt\n",
      "Read data from path: Data/Timestamp/Germany_2019.txt\n",
      "Read data from path: Data/Timestamp/Germany_2020.txt\n",
      "Read data from path: Data/Timestamp/Germany_2021.txt\n",
      "Read data from path: Data/Timestamp/Japan_pre2018.txt\n",
      "Read data from path: Data/Timestamp/Japan_2018.txt\n",
      "Read data from path: Data/Timestamp/Japan_2019.txt\n",
      "Read data from path: Data/Timestamp/Japan_2020.txt\n",
      "Read data from path: Data/Timestamp/Japan_2021.txt\n",
      "Read data from path: Data/Timestamp/Korea_pre2018.txt\n",
      "Read data from path: Data/Timestamp/Korea_2018.txt\n",
      "Read data from path: Data/Timestamp/Korea_2019.txt\n",
      "Read data from path: Data/Timestamp/Korea_2020.txt\n",
      "Read data from path: Data/Timestamp/Korea_2021.txt\n",
      "Read data from path: Data/Timestamp/Vietnam_pre2018.txt\n",
      "Read data from path: Data/Timestamp/Vietnam_2018.txt\n",
      "Read data from path: Data/Timestamp/Vietnam_2019.txt\n",
      "Read data from path: Data/Timestamp/Vietnam_2020.txt\n",
      "Read data from path: Data/Timestamp/Vietnam_2021.txt\n"
     ]
    }
   ],
   "source": [
    "for country in COUNTRY:\n",
    "    for timestamp in TIMESTAMP:\n",
    "        train(country=country, timestamp=timestamp, test_size=0.2, min_topic=40, max_topic=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ca5bb224-c22f-42cd-9429-62b39a3db6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_output(topic_data, country=\"China\", timestamp=\"2021\", test_size=0.2):\n",
    "    # read data\n",
    "    data_file = \"Data/Timestamp/\" + country + \"_\" + timestamp + \".txt\"\n",
    "    with open(data_file) as file:\n",
    "        lines = file.readlines()\n",
    "    print(\"Read data from path: \" + data_file)\n",
    "    \n",
    "    # tokenize\n",
    "    corpus = [list(gensim.utils.tokenize(line, deacc = True)) for line in lines]\n",
    "    \n",
    "    # get corpus\n",
    "    corpus = pd.DataFrame({'reviews': corpus})\n",
    "    docs = corpus['reviews']\n",
    "    \n",
    "    # # train test split\n",
    "    train, test = train_test_split(corpus, test_size=test_size)\n",
    "    docs, docs_train, docs_test = corpus['reviews'], train['reviews'], test['reviews']\n",
    "    \n",
    "    # vocabulary / dictionary\n",
    "    dictionary = gensim.corpora.Dictionary(docs)\n",
    "    \n",
    "    # BOW\n",
    "    bow = [dictionary.doc2bow(doc) for doc in docs]\n",
    "    bow_train = [dictionary.doc2bow(doc) for doc in docs_train]\n",
    "    bow_test = [dictionary.doc2bow(doc) for doc in docs_test]\n",
    "    \n",
    "    # tfidf\n",
    "    tfidf = gensim.models.TfidfModel(bow)\n",
    "    tfidf_train = gensim.models.TfidfModel(bow_train)\n",
    "    tfidf_test = gensim.models.TfidfModel(bow_test)\n",
    "\n",
    "    tfidf = tfidf[bow]\n",
    "    tfidf_train = tfidf_train[bow_train]\n",
    "    tfidf_test = tfidf_test[bow_test]\n",
    "    \n",
    "    # number of topics\n",
    "    file_id = \"Data/NMF/Result_timestamp/\" + country + \"_\" + timestamp + \"_result_umass.csv\"\n",
    "\n",
    "    # find topic\n",
    "    row = topic_data.loc[topic_data[\"file\"] == file_id]\n",
    "    no_topics = row[\"topic\"].iloc[0]\n",
    "    \n",
    "    nmf_model_tfidf = gensim.models.nmf.Nmf(tfidf, \n",
    "                                             num_topics=no_topics, \n",
    "                                             id2word = dictionary, \n",
    "                                             passes = 2)\n",
    "    \n",
    "    directory = \"Data/NMF/Output_Timestamp/\" + country + \"_\" + timestamp\n",
    "    file_output = directory  + \"/\" + country + \"_\" + timestamp\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    # plot and save\n",
    "    \n",
    "    plt.figure()\n",
    "    for t in range(nmf_model_tfidf.num_topics):\n",
    "        plt.imshow(WordCloud(background_color='white', colormap='Oranges').fit_words(dict(nmf_model_tfidf.show_topic(t, 200))))\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"Topic #\" + str(t))\n",
    "        plt.savefig(file_output + \"_topic_\" + str(t))\n",
    "        plt.clf()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0e1e26e3-c137-449c-aa48-3ab9e1215aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read data from path: Data/Timestamp/China_pre2018.txt\n",
      "Read data from path: Data/Timestamp/China_2018.txt\n",
      "Read data from path: Data/Timestamp/China_2019.txt\n",
      "Read data from path: Data/Timestamp/China_2020.txt\n",
      "Read data from path: Data/Timestamp/China_2021.txt\n",
      "Read data from path: Data/Timestamp/France_pre2018.txt\n",
      "Read data from path: Data/Timestamp/France_2018.txt\n",
      "Read data from path: Data/Timestamp/France_2019.txt\n",
      "Read data from path: Data/Timestamp/France_2020.txt\n",
      "Read data from path: Data/Timestamp/France_2021.txt\n",
      "Read data from path: Data/Timestamp/Germany_pre2018.txt\n",
      "Read data from path: Data/Timestamp/Germany_2018.txt\n",
      "Read data from path: Data/Timestamp/Germany_2019.txt\n",
      "Read data from path: Data/Timestamp/Germany_2020.txt\n",
      "Read data from path: Data/Timestamp/Germany_2021.txt\n",
      "Read data from path: Data/Timestamp/Japan_pre2018.txt\n",
      "Read data from path: Data/Timestamp/Japan_2018.txt\n",
      "Read data from path: Data/Timestamp/Japan_2019.txt\n",
      "Read data from path: Data/Timestamp/Japan_2020.txt\n",
      "Read data from path: Data/Timestamp/Japan_2021.txt\n",
      "Read data from path: Data/Timestamp/Korea_pre2018.txt\n",
      "Read data from path: Data/Timestamp/Korea_2018.txt\n",
      "Read data from path: Data/Timestamp/Korea_2019.txt\n",
      "Read data from path: Data/Timestamp/Korea_2020.txt\n",
      "Read data from path: Data/Timestamp/Korea_2021.txt\n",
      "Read data from path: Data/Timestamp/Vietnam_pre2018.txt\n",
      "Read data from path: Data/Timestamp/Vietnam_2018.txt\n",
      "Read data from path: Data/Timestamp/Vietnam_2019.txt\n",
      "Read data from path: Data/Timestamp/Vietnam_2020.txt\n",
      "Read data from path: Data/Timestamp/Vietnam_2021.txt\n"
     ]
    }
   ],
   "source": [
    "topic_data = pd.read_csv('NMF_topic_timestamp.txt',delimiter=',')\n",
    "for country in COUNTRY:\n",
    "    for timestamp in TIMESTAMP:\n",
    "        save_output(topic_data=topic_data, country=country, timestamp=timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5d6dda-6cea-4903-85d8-0d72f7080a77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
