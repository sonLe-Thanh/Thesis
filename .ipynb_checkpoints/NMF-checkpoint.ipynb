{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "03d8a7ed-ac54-4cee-8d44-1821055de73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d132cfd4-0f23-4a8c-bc9a-cc86944b1914",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa7ac9db-8532-4a9b-9154-d996ee2b4204",
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNTRY = [\"China\", \"France\", \"Germany\", \"Japan\", \"Korea\", \"Vietnam\"]\n",
    "TIMESTAMP = [\"pre2018\", \"2018\", \"2019\", \"2020\", \"2021\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0add8a65-bec5-4cfc-9eca-045d5d1ba708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(country=\"China\", timestamp=\"2021\", test_size=0.2, min_topic=40, max_topic=100):\n",
    "    # read data\n",
    "    data_file = \"Data/Timestamp/\" + country + \"_\" + timestamp + \".txt\"\n",
    "    with open(data_file) as file:\n",
    "        lines = file.readlines()\n",
    "    print(\"Read data from path: \" + data_file)\n",
    "    \n",
    "    # tokenize\n",
    "    corpus = [list(gensim.utils.tokenize(line, deacc = True)) for line in lines]\n",
    "    \n",
    "    # get corpus\n",
    "    corpus = pd.DataFrame({'reviews': corpus})\n",
    "    docs = corpus['reviews']\n",
    "    \n",
    "    # # train test split\n",
    "    # train, test = train_test_split(corpus, test_size=test_size)\n",
    "    # docs, docs_train, docs_test = corpus['reviews'], train['reviews'], test['reviews']\n",
    "    \n",
    "    # vocabulary / dictionary\n",
    "    dictionary = gensim.corpora.Dictionary(docs)\n",
    "    \n",
    "    # BOW\n",
    "    bow = [dictionary.doc2bow(doc) for doc in docs]\n",
    "    # bow_train = [dictionary.doc2bow(doc) for doc in docs_train]\n",
    "    # bow_test = [dictionary.doc2bow(doc) for doc in docs_test]\n",
    "    \n",
    "    # tfidf\n",
    "    tfidf = gensim.models.TfidfModel(bow)\n",
    "    # tfidf_train = gensim.models.TfidfModel(bow_train)\n",
    "    # tfidf_test = gensim.models.TfidfModel(bow_test)\n",
    "\n",
    "    tfidf = tfidf[bow]\n",
    "    # tfidf_train = tfidf_train[bow_train]\n",
    "    # tfidf_test = tfidf_test[bow_test]\n",
    "    \n",
    "    # set up file for saving data\n",
    "    directory = \"Data/NMF/Result_timestamp/\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    file_name_umass = directory + country + \"_\" + timestamp + \"_result_umass.csv\"\n",
    "    # file_name_perplexity = directory + country + \"_\" + timestamp + \"_result_perplexity.csv\"\n",
    "    # file_name_perplexity_train = \"Data/Result_timestamp/\" + country + \"_\" + timestamp + \"_\" + str(no_iterations) +\"_result_perplexity_train.csv\"\n",
    "    # file_name_perplexity_test = \"Data/Result_timestamp/\" + country + \"_\" + timestamp + \"_\" + str(no_iterations) +\"_result_perplexity_test.csv\"\n",
    "\n",
    "    file_write_umass = open(file_name_umass, \"a\")\n",
    "    # file_write_perplexity = open(file_name_perplexity, \"a\")\n",
    "    # file_write_perplexity_train = open(file_name_perplexity_train, \"a\")\n",
    "    # file_write_perplexity_test = open(file_name_perplexity_test, \"a\")\n",
    "\n",
    "    for no_topics in range(min_topic,max_topic + 1):\n",
    "\n",
    "        start = time.time()\n",
    "        nmf_model_tfidf = gensim.models.nmf.Nmf(tfidf, \n",
    "                                                 num_topics=no_topics, \n",
    "                                                 id2word = dictionary, \n",
    "                                                 passes = 2)\n",
    "        end = time.time()\n",
    "        \n",
    "        # save umass result\n",
    "        u_mass = CoherenceModel(model=nmf_model_tfidf, corpus=bow, dictionary=dictionary ,coherence='u_mass', topn=20)\n",
    "        u_mass_res = u_mass.get_coherence() \n",
    "\n",
    "        # save perplexity result\n",
    "        # perplexity = nmf_model_tfidf.log_perplexity(tfidf, len(docs))\n",
    "        # perplexity_train = lda_model_tfidf.log_perplexity(tfidf_train, len(docs_train))\n",
    "        # perplexity_test = lda_model_tfidf.log_perplexity(tfidf_test, len(docs_test))\n",
    "        \n",
    "        # store data to file\n",
    "        file_write_umass.write(str(no_topics) + \",\" + str(u_mass_res) +\",\"+str(end-start)+\"\\n\")\n",
    "        # file_write_perplexity.write(str(no_topics) + \",\" + str(perplexity) +\",\"+str(end - start)+\"\\n\")\n",
    "        # file_write_perplexity_train.write(str(no_topics) + \",\" + str(perplexity_train) +\",\"+str(end - start)+\"\\n\")\n",
    "        # file_write_perplexity_test.write(str(no_topics) + \",\" + str(perplexity_test) +\",\"+str(end - start)+\"\\n\")\n",
    "\n",
    "    file_write_umass.close()\n",
    "    # file_write_perplexity.close()\n",
    "    # file_write_perplexity_train.close()\n",
    "    # file_write_perplexity_test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4790295c-9687-4285-8fc2-ceb1ce11eb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read data from path: Data/Timestamp/China_pre2018.txt\n",
      "Read data from path: Data/Timestamp/China_2018.txt\n",
      "Read data from path: Data/Timestamp/China_2019.txt\n",
      "Read data from path: Data/Timestamp/China_2020.txt\n",
      "Read data from path: Data/Timestamp/China_2021.txt\n",
      "Read data from path: Data/Timestamp/France_pre2018.txt\n",
      "Read data from path: Data/Timestamp/France_2018.txt\n",
      "Read data from path: Data/Timestamp/France_2019.txt\n",
      "Read data from path: Data/Timestamp/France_2020.txt\n",
      "Read data from path: Data/Timestamp/France_2021.txt\n",
      "Read data from path: Data/Timestamp/Germany_pre2018.txt\n",
      "Read data from path: Data/Timestamp/Germany_2018.txt\n",
      "Read data from path: Data/Timestamp/Germany_2019.txt\n",
      "Read data from path: Data/Timestamp/Germany_2020.txt\n",
      "Read data from path: Data/Timestamp/Germany_2021.txt\n",
      "Read data from path: Data/Timestamp/Japan_pre2018.txt\n",
      "Read data from path: Data/Timestamp/Japan_2018.txt\n",
      "Read data from path: Data/Timestamp/Japan_2019.txt\n"
     ]
    }
   ],
   "source": [
    "for country in COUNTRY:\n",
    "    for timestamp in TIMESTAMP:\n",
    "        train(country=country, timestamp=timestamp, test_size=0.2, min_topic=40, max_topic=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca5bb224-c22f-42cd-9429-62b39a3db6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_output(topic_data, country=\"China\", timestamp=\"2021\", no_iterations=50, test_size=0.2):\n",
    "    # read data\n",
    "    data_file = \"Data/Timestamp/\" + country + \"_\" + timestamp + \".txt\"\n",
    "    with open(data_file) as file:\n",
    "        lines = file.readlines()\n",
    "    print(\"Read data from path: \" + data_file)\n",
    "    \n",
    "    # tokenize\n",
    "    corpus = [list(gensim.utils.tokenize(line, deacc = True)) for line in lines]\n",
    "    \n",
    "    # get corpus\n",
    "    corpus = pd.DataFrame({'reviews': corpus})\n",
    "    docs = corpus['reviews']\n",
    "    \n",
    "    # # train test split\n",
    "    train, test = train_test_split(corpus, test_size=test_size)\n",
    "    docs, docs_train, docs_test = corpus['reviews'], train['reviews'], test['reviews']\n",
    "    \n",
    "    # vocabulary / dictionary\n",
    "    dictionary = gensim.corpora.Dictionary(docs)\n",
    "    \n",
    "    # BOW\n",
    "    bow = [dictionary.doc2bow(doc) for doc in docs]\n",
    "    bow_train = [dictionary.doc2bow(doc) for doc in docs_train]\n",
    "    bow_test = [dictionary.doc2bow(doc) for doc in docs_test]\n",
    "    \n",
    "    # tfidf\n",
    "    tfidf = gensim.models.TfidfModel(bow)\n",
    "    tfidf_train = gensim.models.TfidfModel(bow_train)\n",
    "    tfidf_test = gensim.models.TfidfModel(bow_test)\n",
    "\n",
    "    tfidf = tfidf[bow]\n",
    "    tfidf_train = tfidf_train[bow_train]\n",
    "    tfidf_test = tfidf_test[bow_test]\n",
    "    \n",
    "    # number of topics\n",
    "    file_id = \"Data/Result_timestamp/\" + country + \"_\" + timestamp + \"_\" + str(no_iterations) +\"_result_umass.csv\"\n",
    "\n",
    "    # find topic\n",
    "    row = topic_data.loc[topic_data[\"file\"] == file_id]\n",
    "    no_topics = row[\"topic\"]\n",
    "    \n",
    "    nmf_model_tfidf = gensim.models.nmf.Nmf(tfidf, \n",
    "                                             num_topics=no_topics, \n",
    "                                             id2word = dictionary, \n",
    "                                             passes = 2)\n",
    "    \n",
    "    directory = \"Data/NMF_Output/Timestamp/\" + country + \"_\" + timestamp\n",
    "    file_output = directory  + \"/\" + country + \"_\" + timestamp\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    # plot and save\n",
    "    \n",
    "    plt.figure()\n",
    "    for t in range(nmf_model_tfidf.num_topics):\n",
    "        plt.imshow(WordCloud(background_color='white', colormap='Oranges').fit_words(dict(nmf_model_tfidf.show_topic(t, 200))))\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"Topic #\" + str(t))\n",
    "        plt.savefig(file_output + \"_topic_\" + str(t), dpi=1000)\n",
    "        plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e1e26e3-c137-449c-aa48-3ab9e1215aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read data from path: Data/Timestamp/China_pre2018.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thuytdv/miniforge3/envs/PycharmProjects/lib/python3.9/site-packages/numpy/core/shape_base.py:65: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  ary = asanyarray(ary)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'Series' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m country \u001b[38;5;129;01min\u001b[39;00m COUNTRY:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m timestamp \u001b[38;5;129;01min\u001b[39;00m TIMESTAMP:\n\u001b[0;32m----> 4\u001b[0m         \u001b[43msave_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopic_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtopic_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcountry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcountry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestamp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestamp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36msave_output\u001b[0;34m(topic_data, country, timestamp, no_iterations, test_size)\u001b[0m\n\u001b[1;32m     40\u001b[0m row \u001b[38;5;241m=\u001b[39m topic_data\u001b[38;5;241m.\u001b[39mloc[topic_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m file_id]\n\u001b[1;32m     41\u001b[0m no_topics \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopic\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 43\u001b[0m nmf_model_tfidf \u001b[38;5;241m=\u001b[39m \u001b[43mgensim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnmf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNmf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtfidf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mnum_topics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_topics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mid2word\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdictionary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mpasses\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData/NMF_Output/Timestamp/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m country \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m timestamp\n\u001b[1;32m     49\u001b[0m file_output \u001b[38;5;241m=\u001b[39m directory  \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m country \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m timestamp\n",
      "File \u001b[0;32m~/miniforge3/envs/PycharmProjects/lib/python3.9/site-packages/gensim/models/nmf.py:219\u001b[0m, in \u001b[0;36mNmf.__init__\u001b[0;34m(self, corpus, num_topics, id2word, chunksize, passes, kappa, minimum_probability, w_max_iter, w_stop_condition, h_max_iter, h_stop_condition, eval_every, normalize, random_state)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m corpus \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 219\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/PycharmProjects/lib/python3.9/site-packages/gensim/models/nmf.py:649\u001b[0m, in \u001b[0;36mNmf.update\u001b[0;34m(self, corpus, chunksize, passes, eval_every)\u001b[0m\n\u001b[1;32m    640\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPROGRESS: pass \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m, at document #\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    642\u001b[0m         pass_, chunk_idx \u001b[38;5;241m*\u001b[39m chunksize \u001b[38;5;241m+\u001b[39m chunk_len, lencorpus\n\u001b[1;32m    643\u001b[0m     )\n\u001b[1;32m    645\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_W \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    646\u001b[0m     \u001b[38;5;66;03m# If `self._W` is not set (i.e. the first batch being handled), compute the initial matrix using the\u001b[39;00m\n\u001b[1;32m    647\u001b[0m     \u001b[38;5;66;03m# batch mean.\u001b[39;00m\n\u001b[0;32m--> 649\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_solveproj(v, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_W, h\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h, v_max\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_max)\n\u001b[1;32m    652\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h\n",
      "File \u001b[0;32m~/miniforge3/envs/PycharmProjects/lib/python3.9/site-packages/gensim/models/nmf.py:533\u001b[0m, in \u001b[0;36mNmf._setup\u001b[0;34m(self, v)\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;124;03m\"\"\"Infer info from the first batch and initialize the matrices.\u001b[39;00m\n\u001b[1;32m    522\u001b[0m \n\u001b[1;32m    523\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    527\u001b[0m \n\u001b[1;32m    528\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw_std \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(v\u001b[38;5;241m.\u001b[39mmean() \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_tokens \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_topics))\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_W \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw_std\n\u001b[0;32m--> 533\u001b[0m     \u001b[38;5;241m*\u001b[39m \u001b[43mhalfnorm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrvs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m        \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_topics\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    536\u001b[0m )\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mA \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_topics, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_topics))\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mB \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_tokens, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_topics))\n",
      "File \u001b[0;32m~/miniforge3/envs/PycharmProjects/lib/python3.9/site-packages/scipy/stats/_distn_infrastructure.py:1092\u001b[0m, in \u001b[0;36mrv_generic.rvs\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m   1090\u001b[0m     vals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rvs(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1092\u001b[0m     vals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rvs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1094\u001b[0m vals \u001b[38;5;241m=\u001b[39m vals \u001b[38;5;241m*\u001b[39m scale \u001b[38;5;241m+\u001b[39m loc\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;66;03m# do not forget to restore the _random_state\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/PycharmProjects/lib/python3.9/site-packages/scipy/stats/_continuous_distns.py:3505\u001b[0m, in \u001b[0;36mhalfnorm_gen._rvs\u001b[0;34m(self, size, random_state)\u001b[0m\n\u001b[1;32m   3504\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_rvs\u001b[39m(\u001b[38;5;28mself\u001b[39m, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m-> 3505\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstandard_normal\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32mmtrand.pyx:1400\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.standard_normal\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_common.pyx:598\u001b[0m, in \u001b[0;36mnumpy.random._common.cont\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Series' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "topic_data = pd.read_csv('topic_timestamp.txt',delimiter=',')\n",
    "for country in COUNTRY:\n",
    "    for timestamp in TIMESTAMP:\n",
    "        save_output(topic_data=topic_data, country=country, timestamp=timestamp, no_iterations=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5d6dda-6cea-4903-85d8-0d72f7080a77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
